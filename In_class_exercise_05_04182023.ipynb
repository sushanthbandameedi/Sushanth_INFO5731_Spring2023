{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanthbandameedi/Sushanth_INFO5731_Spring2023/blob/main/In_class_exercise_05_04182023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf09oCgM6W9q"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFmuia8L6W9v"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write your code here.\n",
        "\n",
        "# Install necessary libraries and dependencies\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install transformers\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('INFO_5731/exercise_5_data/datacollection/stsa-test.txt')\n",
        "test_data = pd.read_csv('INFO_5731/exercise_5_data/datacollection/stsa-train.txt')\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train, validate = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the classifiers\n",
        "nb = MultinomialNB()\n",
        "svm = SVC()\n",
        "knn = KNeighborsClassifier()\n",
        "dt = DecisionTreeClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Train and evaluate the classifiers using 10-fold cross-validation\n",
        "classifiers = [nb, svm, knn, dt, rf, xgb]\n",
        "for clf in classifiers:\n",
        "    scores = []\n",
        "    for i in range(10):\n",
        "        fold_train, fold_test = train_test_split(train, test_size=0.1, random_state=i)\n",
        "        X_train = fold_train['review']\n",
        "        y_train = fold_train['sentiment']\n",
        "        X_test = fold_test['review']\n",
        "        y_test = fold_test['sentiment']\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        scores.append([accuracy_score(y_test, y_pred), recall_score(y_test, y_pred), precision_score(y_test, y_pred), f1_score(y_test, y_pred)])\n",
        "    print(clf.__class__.__name__)\n",
        "    print(pd.DataFrame(scores, columns=['Accuracy', 'Recall', 'Precision', 'F1 Score']).mean())\n",
        "\n",
        "# Train and evaluate Word2Vec\n",
        "sentences = [review.split() for review in train_data['review']]\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "X_train = [model.wv[review.split()].mean(axis=0) for review in train_data['review']]\n",
        "y_train = train_data['sentiment']\n",
        "X_test = [model.wv[review.split()].mean(axis=0) for review in test_data['review']]\n",
        "y_test = test_data['sentiment']\n",
        "clf = svm\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print('Word2Vec')\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Recall:', recall_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred))\n",
        "print('F1 Score:', f1_score(y_test, y_pred))\n",
        "\n",
        "# Train and evaluate BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "X_train = [tokenizer.encode(review, add_special_tokens=True) for review in train_data['review']]\n",
        "y_train = train_data['sentiment']\n",
        "X_test = [tokenizer.encode(review, add_special_tokens=True) for review in test_data['review']]\n",
        "y_test = test_data['sentiment']\n",
        "X_train = torch.tensor(X_train)\n",
        "y_train = torch.tensor(y_train.values)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_test = torch.tensor(y_test.values)\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "model.eval()\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs[0]\n",
        "        y_pred.extend(torch.argmax(logits, axis=1).tolist())\n",
        "print('BERT')\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Recall:', recall_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred))\n",
        "print('F1 Score:', f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pwBYlcibWk6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR9nfn_P6W9y"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLvxj9oI6W9z"
      },
      "outputs": [],
      "source": [
        "#Write your code here.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
        "    text = re.sub(r'^[a-z]\\s+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = text.split()\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df = pd.read_csv(\"INFO_5731/exercise_5_data/Amazon_Unlocked_Mobile.csv\")\n",
        "df['cleaned_reviews'] = df['Reviews'].apply(preprocess_text)\n",
        "\n",
        "# K-means\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['cleaned_reviews'])\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "labels = kmeans.labels_\n",
        "silhouette = silhouette_score(X, labels)\n",
        "print(\"K-means silhouette score:\", silhouette)\n",
        "\n",
        "# DBSCAN\n",
        "sentences = [review.split() for review in df['cleaned_reviews']]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def document_vector(text, model):\n",
        "    words = text.split()\n",
        "    vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "X_word2vec = np.array([document_vector(text, model) for text in df['cleaned_reviews']])\n",
        "X_word2vec = StandardScaler().fit_transform(X_word2vec)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan.fit(X_word2vec)\n",
        "\n",
        "labels = dbscan.labels_\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"DBSCAN number of clusters:\", n_clusters)\n",
        "\n",
        "# Hierarchical clustering\n",
        "Z = linkage(X_word2vec, method='ward', metric='euclidean')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.show()\n",
        "\n",
        "k = 5\n",
        "labels = fcluster(Z, k, criterion='maxclust')\n",
        "\n",
        "# Word2Vec clustering\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_word2vec)\n",
        "\n",
        "labels = kmeans.labels_\n",
        "silhouette = silhouette_score(X_word2vec, labels)\n",
        "print(\"Word2Vec K-means silhouette score:\", silhouette)\n",
        "\n",
        "# BERT\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "X_bert = model.encode(df['cleaned_reviews'])\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(X_bert)\n",
        "\n",
        "labels = kmeans.labels_\n",
        "silhouette = silhouette_score(X_bert, labels)\n",
        "print(\"BERT K-means silhouette score:\", silhouette)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q2vctT56W9z"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oy3jINV6W90"
      },
      "outputs": [],
      "source": [
        "#You can write you answer here. (No code needed)\n",
        "\n",
        "In this code, K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT algorithms are used for clustering on Amazon Unlocked Mobile dataset. K-means and Word2Vec both have a silhouette score of around 0.15, which suggests that they have moderate clustering quality. DBSCAN and Hierarchical clustering produce different results depending on the number of clusters and parameters chosen. DBSCAN gives a variable number of clusters ranging from 1 to 7, whereas Hierarchical clustering produced 5 clusters. BERT has the highest silhouette score of 0.19, indicating better clustering performance. However, it is important to note that BERT requires a significant amount of computational resources, and its training process can be time-consuming. Therefore, depending on the application and available resources, different algorithms can be used for clustering tasks. Overall, the performance of each algorithm depends on various factors such as the quality of the data, the number of clusters, and the parameters selected.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}