{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanthbandameedi/Sushanth_INFO5731_Spring2023/blob/main/In_class_exercise_03_02282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWFrUp-cLElf"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqUqHnsiLElj"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3f93d4XLElk"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP6qXEqkLEll"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "An interesting text classification or text mining task could be sentiment analysis of movie reviews. The task would involve predicting whether a review is positive or negative based on its text. Some useful features for building the machine learning model could include:\n",
        "\n",
        "Bag-of-words - The presence or absence of specific words in a movie review can provide a valuable signal for sentiment analysis. For example, the presence of words like \"enjoyable,\" \"great,\" and \"excellent\" could indicate a positive sentiment, while the presence of words like \"boring,\" \"terrible,\" and \"disappointing\" could indicate a negative sentiment.\n",
        "\n",
        "N-grams - N-grams are sequences of words in a text, and can be used to capture more complex patterns in language. For example, a 2-gram \"not good\" could indicate a negative sentiment, while a 2-gram \"really good\" could indicate a positive sentiment.\n",
        "\n",
        "Part-of-speech tags - Identifying the parts of speech in a movie review can provide additional context and meaning to the text. For example, adjectives and adverbs can be strong indicators of sentiment.\n",
        "\n",
        "Sentiment lexicons - Lexicons are lists of words with associated sentiment scores. They can be used to calculate a sentiment score for a movie review based on the sentiment scores of the individual words in the text.\n",
        "\n",
        "Word embeddings - Word embeddings are dense vector representations of words that capture the semantic relationships between them. They can be used to capture more nuanced meanings and relationships between words in a movie review.\n",
        "\n",
        "Named entities: Named entities, such as person names or movie titles, could be identified using tools such as spaCy. These features could be useful for capturing the subject matter of the review, which could be important for predicting sentiment.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "56Qrwx_AsKmz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbEHVyplLElm"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KNAUg233LElm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec7564f-a200-4b36-f103-bacc191dec04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/TextClassification/movie_reviews_dataset.csv')\n",
        "\n",
        "\n",
        "# Define lists to store the features and labels\n",
        "index = []\n",
        "count = []\n",
        "hate_speech = []\n",
        "offensive_language = []\n",
        "neither = []\n",
        "class_label = []\n",
        "tweets = []\n",
        "\n",
        "# Loop through the dataset and extract the features\n",
        "for i, row in df.iterrows():\n",
        "    index.append(row['index'])\n",
        "    count.append(row['count'])\n",
        "    hate_speech.append(row['hate_speech'])\n",
        "    offensive_language.append(row['offensive_language'])\n",
        "    neither.append(row['neither'])\n",
        "    class_label.append(row['class'])\n",
        "    tweets.append(row['tweet'])\n",
        "\n",
        "# Convert the feature lists to numpy arrays\n",
        "index = np.array(index)\n",
        "count = np.array(count)\n",
        "hate_speech = np.array(hate_speech)\n",
        "offensive_language = np.array(offensive_language)\n",
        "neither = np.array(neither)\n",
        "class_label = np.array(class_label)\n",
        "tweets = np.array(tweets)\n",
        "\n",
        "# Print the shapes of the feature arrays\n",
        "print('Index shape:', index.shape)\n",
        "print('Count shape:', count.shape)\n",
        "print('Hate speech shape:', hate_speech.shape)\n",
        "print('Offensive language shape:', offensive_language.shape)\n",
        "print('Neither shape:', neither.shape)\n",
        "print('Class label shape:', class_label.shape)\n",
        "print('Tweets shape:', tweets.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRhGlrnILEln"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pkq0W715LEln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "60e04d33-17b5-4cb8-c26d-8d7af2cc0223"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d91c17d60f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Defining the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/TextClassification/movie_reviews_dataset.csv')\n",
        "\n",
        "# create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# fit and transform the text data\n",
        "X = vectorizer.fit_transform(df['tweet'])\n",
        "\n",
        "# select the top 5,000 features using the chi-squared test\n",
        "k = 5000\n",
        "selector = SelectKBest(chi2, k)\n",
        "selector.fit(X, df['class'])\n",
        "\n",
        "# get the names of the selected features\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "selected_feature_names = [feature_names[i] for i in selector.get_support(indices=True)]\n",
        "\n",
        "# print the top 10 selected features\n",
        "print(\"Top 10 selected features:\")\n",
        "for feature in selected_feature_names[:10]:\n",
        "    print(feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMrgMj9ULEln"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Pb0ArszLElo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "724c439c-aecb-4550-c081-f9b48dcdfac4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-706dc9c24e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/TextClassification/movie_reviews_dataset.csv')\n",
        "\n",
        "# create a BERT model object\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# create a query\n",
        "query = 'best action movies of all time'\n",
        "\n",
        "# encode the query with the BERT model\n",
        "query_embedding = model.encode([query])[0]\n",
        "\n",
        "# encode the text data with the BERT model\n",
        "text_embeddings = model.encode(df['tweet'].tolist())\n",
        "\n",
        "# calculate the cosine similarity between the query and each text\n",
        "similarities = cosine_similarity([query_embedding], text_embeddings)\n",
        "\n",
        "# create a new dataframe with the similarities and the original text data\n",
        "result_df = pd.DataFrame({'text': df['tweet'], 'similarity': similarities[0]})\n",
        "\n",
        "# sort the dataframe by similarity in descending order\n",
        "result_df = result_df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "# print the top 10 results\n",
        "print(result_df.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}