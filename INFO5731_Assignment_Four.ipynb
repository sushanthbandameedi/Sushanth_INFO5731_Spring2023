{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushanthbandameedi/Sushanth_INFO5731_Spring2023/blob/main/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('INFO_5731/assgn_3/final.csv')\n",
        "\n",
        "# Create the document-term matrix\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "doc_term_matrix = vectorizer.fit_transform(df['review_text'])\n",
        "\n",
        "# Fit the LDA model\n",
        "num_topics = 10\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda_model.fit(doc_term_matrix)\n",
        "\n",
        "# Print the top 10 clusters for topic modeling\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "top_clusters = {}\n",
        "for i, topic in enumerate(lda_model.components_):\n",
        "    top_clusters[f\"Cluster {i+1}\"] = [feature_names[idx] for idx in topic.argsort()[-10:][::-1]]\n",
        "    print(f\"Cluster {i+1}: {top_clusters[f'Cluster {i+1}']}\")\n",
        "\n",
        "# Summarize and describe the topic for each cluster\n",
        "for cluster, words in top_clusters.items():\n",
        "    print(f\"\\n{cluster}:\")\n",
        "    for word in words:\n",
        "        print(f\"- {word}\")\n",
        "    print(\"\\nTopic summary:\")\n",
        "    for index, row in enumerate(lda_model.components_):\n",
        "        top_words = [feature_names[i] for i in row.argsort()[:-11:-1]]\n",
        "        if all(word in top_words for word in words):\n",
        "            print(df['review_text'][lda_model.transform(doc_term_matrix)[index].argmax()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('INFO_5731/assgn_3/final.csv')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review_text'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# Model 1: Multinomial Naive Bayes\n",
        "mnb_model = MultinomialNB()\n",
        "mnb_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Cross-validation for MNB model\n",
        "mnb_scores = cross_val_score(mnb_model, X_train_features, y_train, cv=5)\n",
        "print(\"Multinomial Naive Bayes cross-validation scores:\", mnb_scores)\n",
        "\n",
        "# Model 2: Logistic Regression\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Cross-validation for Logistic Regression model\n",
        "lr_scores = cross_val_score(lr_model, X_train_features, y_train, cv=5)\n",
        "print(\"Logistic Regression cross-validation scores:\", lr_scores)\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "mnb_pred = mnb_model.predict(X_test_features)\n",
        "lr_pred = lr_model.predict(X_test_features)\n",
        "\n",
        "print(\"Multinomial Naive Bayes performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, mnb_pred))\n",
        "print(\"Precision:\", precision_score(y_test, mnb_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, mnb_pred, average='weighted'))\n",
        "print(\"F1 score:\", f1_score(y_test, mnb_pred, average='weighted'))\n",
        "\n",
        "print(\"\\nLogistic Regression performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))\n",
        "print(\"Precision:\", precision_score(y_test, lr_pred, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, lr_pred, average='weighted'))\n",
        "print(\"F1 score:\", f1_score(y_test, lr_pred, average='weighted'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv('INFO_5731/assgn_4/ques_3/train.csv')\n",
        "test_df = pd.read_csv('INFO_5731/assgn_4/ques_3/test.csv')\n",
        "\n",
        "# Load the data description file\n",
        "with open('INFO_5731/assgn_4/ques_3/data_description.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    desc_dict = {}\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if ':' in line:\n",
        "            key, val = line.split(':')\n",
        "            desc_dict[key.strip()] = val.strip()\n",
        "\n",
        "# Preprocessing: Impute missing values and one-hot encoding\n",
        "combined_df = pd.concat([train_df.drop('SalePrice', axis=1), test_df], axis=0, ignore_index=True)\n",
        "for col in combined_df.columns:\n",
        "    if combined_df[col].dtype == 'object':\n",
        "        combined_df[col].fillna(desc_dict[col], inplace=True)\n",
        "    else:\n",
        "        combined_df[col].fillna(combined_df[col].mean(), inplace=True)\n",
        "combined_df = pd.get_dummies(combined_df)\n",
        "\n",
        "# Split the datasets\n",
        "X_train = combined_df[:train_df.shape[0]]\n",
        "X_test = combined_df[train_df.shape[0]:]\n",
        "y_train = train_df['SalePrice']\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test_split)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test_split, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_split, y_pred)\n",
        "\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}